{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534dce8c",
   "metadata": {},
   "source": [
    "## Logistic Regression (C1W2L02)\n",
    "\n",
    "Given $x$ we want:\n",
    "\n",
    "$$\n",
    "\\hat{y} = P(y = 1 \\mid x)\n",
    "$$\n",
    "\n",
    "- $x$ is the input feature \n",
    "- $\\hat{y}$ is the predicted probability produced by the model (it's not the true value). It represents the model estimate.\n",
    "- $P(y = 1 \\mid x)$ is the conditional probability, i.e., the probability that $y$ equals 1 given $x$.\n",
    "- $y$ is the true label (0 or 1), the ground truth. This parameter comes from the dataset.\n",
    "\n",
    "Example to illustrate $y$:\n",
    "\n",
    "| Pixel (R) | y |\n",
    "| --------- | - |\n",
    "| 255       | 1 |\n",
    "| 240       | 1 |\n",
    "| 210       | 1 |\n",
    "| 180       | 0 |\n",
    "| 90        | 0 |\n",
    "\n",
    "In this example, we have a simple dataset where the input feature is the red channel value of a pixel (ranging from 0 to 255), and the corresponding label $y$ indicates whether the pixel is classified as belonging to the positive class (1) or not (0) (or whether it's bright or dark).\n",
    "\n",
    "We need to find a way to adjust the output so that it can be interpreted as a probability between 0 and 1. This is done by using the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Our model can be represented as:\n",
    "$$\n",
    "\\hat{y} = \\sigma(w^T x + b)\n",
    "$$\n",
    "\n",
    "The sigmoid function is fixed, it receives an input $z$ and outputs a value between 0 and 1. The parameters we can adjust are $w$ (weights) and $b$ (bias) before applying the sigmoid function.\n",
    "\n",
    "## Logistic Regression Cost Function (C1W2L03)\n",
    "\n",
    "Cost / Loss function measures how well the model's predictions match the true labels. It quantifies the difference between the predicted probabilities $\\hat{y}$ and the actual labels $y$.\n",
    "\n",
    "The most common cost function used is the quadratic cost function, but in the example Andrew Ng uses, it's not ideal for logistic regression  because it can lead to non-convex optimization problem.\n",
    "\n",
    "Instead, we use the logistic loss (also known as log loss or binary cross-entropy loss):\n",
    "$$\n",
    "f(\\hat{y},y) = -y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})\n",
    "$$\n",
    "\n",
    "If $y$ is 1, only the first part of the equation matters, if $y$ is 0, only the second part matters.\n",
    "\n",
    "Let's say $y$ = 1 and $\\hat{y}$ = 0.7:\n",
    "\n",
    "$$\n",
    "f(0.7, 1) = -1 \\cdot \\log(0.7) - (1-1) \\cdot \\log(1-0.7)\n",
    "$$\n",
    "$$\n",
    "= -1 \\cdot \\log(0.7) - 0 \\cdot \\log(0.3) \n",
    "$$\n",
    "\n",
    "Here the second part doesn't matter because it's multiplied by 0 now.\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "f(0.7, 1) = -\\log(0.7)\n",
    "$$\n",
    "$$\n",
    "\\approx 0.3567\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
